# Load the data set

For this course I provided the data set of video games sold by an imaginary video game retailer. We'll use this data to train the neural network that will predict how much money we can expect future video games to earn based on our historical data. First, let's open up the data and take a look at it in the spreadsheet application. The data is in a file called sales_data_training.csv. I've already opened it up here in my spreadsheet application. In this data set we have one row for each video game title that our store has sold in the past. For each video game, we've recorded several attributes. First, we have critic_rating, which is an average star rating out of the five stars. Next, is_action, which tells us if this is an action game, is_exclusive_to_us, which tells us if we have an exclusive deal to sell this game, is_portable, which tells us if this game runs on the handheld video game player, is_role_playing, which tells us this is a role-playing game, which is a genre of video games, is_sequel, which tells us this game was a sequel of an earlier video game, is_sports, which tells us this is a sports video game, suitable_for_kids, which tells us this game is appropriate for all ages. We also have total_earnings, which is how much total money we earned from selling this game to all customers, and unit_price, which tells us how much money one copy of the game retailed for. We'll use TensorFlow to build the neural network that tries to predict the total earnings of a new game, based on these characteristics. Along with this file, we also have a second data file called sales_data_test.csv. I'm going to open it in the spreadsheet, so we're going to flip to that now. Here we go. This file is exactly like the first one, it just has different data. I've gone ahead and split the data into a training data set and a test data set. The machine learning system will only get to see the training data set during the training phase. Then we'll use this test data to check the accuracy of the predictions from our neural network. Let's open up load_data.py, and write the code to load this data. Let's flip to PyCharm, and we'll open up 03, load_data.py. First, we're going to use pandas to read the training data. Pandas is a Python library that makes it easy to load data into memory and work with the data as if it were in a virtual spreadsheet. You can do many of the same things as you do with a spreadsheet, like sorting data, pulling out specific columns of data, or applying calculations. Our training data set is stored in the file called sales_data_training.csv. We can load it by calling the pandas read csv function, pd.read_csv, and passing in the file name we want to load, sales_data_training.csv. Pandas will then load the data into a DataFrame. To prevent a warning from showing up at runtime, we can also pass in a dtype parameter, to tell it explicitly what type of data we are loading, so I'll pass in dtype=float, to tell it we are expecting floating-point numbers. Next we need to split the training data into two groups, the X group is data about each video game that we'll pass into the neural network, and the Y group are the values we want to predict. In this case, that's how much money we think each video game will earn. Let's start on line 9. To create the X training data, we'll take all the columns in the training data, and simply drop the total_earnings column. That will give us all the columns except the one we don't want. So we'll call training_data_df.drop, and we'll pass in the name of the column to drop, total_earnings. Then we also need to pass in an axis=1 parameter that tells it we want to drop a column and not a row. Finally, we'll call .values to get back the result as an array. To create the Y array, we will only grab the total_earnings column, so we'll call training_data_df, and then we'll pass in the total_earnings column, and then we'll call .values to get the result as an array. Now we need to repeat the exact same process to load the test data set. Again, we'll use pandas read_csv function to load the data. So on line 13, pd.read_csv, and we'll pass in the name of the file, sales_data_test.csv, and we'll in dtype=float. And again, we'll drop the total_earnings column to get the X array, so we'll say, on line 16, x_testing = test_data_df.drop, and we'll pass in the name of the column to drop, total_earnings, and axis=1, and finally we'll call .values to get the result as an array, and we'll pull out the total_earnings column to create the Y array, test_data_df, and pass in the name of the column to grab, total_earnings, and then we'll call .values to get the result as an array. Now we need to pre-process our data. In order to train the neural network, we want to scale all the numbers in each column of our data set to be between the value of 0 and 1. If the numbers in one column are large but the numbers in another column are small, the neural network training won't work very well. One way we can do this is to use the MinMaxScaler object from the popular scikit-learn library. It's designed for exactly this purpose. So on line 21, first we'll create a new MinMaxScaler. We just need to pass in a feature_range parameter, which tells it that we want all numbers scaled between 0 and 1. So we'll say feature_range= and then pass in a tuple (0, 1), and then we'll do the same thing for the Y_scaler MinMaxScaler(feature_range=(0, 1)) To actually scale our data, we call the fit_transform function on the scaler object, and pass in the data we want to scale. The result will be the rescaled data. So on line 25, we'll call X_scaler.fit_transform, and we'll pass in X_training as the data to scale. And then we'll do the same for the Y_scaled_training data on line 26. We'll say Y_scaler.fit_transform, and we'll pass in the Y_training data. Fit_transform means we want it to first fit to our data, or figure out how much to scale down the numbers in each column, and then we want it to actually transform, or scale the data. Now we need to scale the test data in the same way. We want to make sure the test data is scaled by the same amount as the training data. So in this case, we'll just call transform on each scaler object, instead of fit_transform. So on line 29, we'll call X_scaler.transform, and we'll pass in the X_testing data, and then on line 30, Y_scaler.transform, and we'll pass in the Y_testing data. And now our data is ready to pass in the TensorFlow. The scaler scales the data by multiplying it by a constant number and adding a constant number. At the bottom here, I printed out how the Y data was scaled. We can get these values by looking at the Y_scaler.scale_[0] Object, and the Y_scaler.min[0] Object. This will be useful to know later, when we want to make predictions with the neural network and be able to unscale the data back to the original units. Okay, we should run the code to make sure it works. But before we run it, I noticed I've made a small mistake on line 9. I wrote total_earning, but there should be an s on that. It should be total_earnings. Let's add an s. Okay, right click and run. And we can see that the data was scaled by these two values.
